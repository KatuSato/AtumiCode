{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from contextlib import contextmanager\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold,train_test_split\n",
    "from catboost import Pool\n",
    "from catboost import CatBoostRegressor,CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# ===============\n",
    "# Feature Engineering\n",
    "# ===============\n",
    "\n",
    "class SinCos():\n",
    "    def __init__(self, feature_name, period):\n",
    "        '''\n",
    "        input\n",
    "        ---\n",
    "        feature_name(str): name of feature\n",
    "        period(int): period of feature\n",
    "        '''\n",
    "        self.feature_name = feature_name\n",
    "        self.period = period\n",
    "\n",
    "    def create_features(self, df):\n",
    "        df['{}_sin'.format(self.feature_name)] = np.sin(2 * np.pi * df[self.feature_name]/self.period)\n",
    "        df['{}_cos'.format(self.feature_name)] = np.cos(2 * np.pi * df[self.feature_name] / self.period)\n",
    "        new_cols = [\"{}_{}\".format(self.feature_name, key) for key in [\"sin\", \"cos\"]]\n",
    "\n",
    "        return df, new_cols\n",
    "\n",
    "\n",
    "class Frequency():\n",
    "    def __init__(self, categorical_columns):\n",
    "        '''\n",
    "        input\n",
    "        ---\n",
    "        categorical_columns(list): categorical columns\n",
    "        '''\n",
    "        self.categorical_columns = categorical_columns\n",
    "\n",
    "    def create_features(self, df):\n",
    "        new_cols = []\n",
    "        for index,col in enumerate(self.categorical_columns):\n",
    "            print(\"======{}/{}=====\".format(index,len(self.categorical_columns)))\n",
    "            fname = '{}_Frequency'.format(col)\n",
    "            df[fname] = df.groupby(col)[col].transform('count') / len(df)\n",
    "            new_cols.append(fname)\n",
    "        return df, new_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "LOGGER = logging.getLogger()\n",
    "FORMATTER = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "def setup_logger(out_file=None, stderr=True, stderr_level=logging.INFO, file_level=logging.DEBUG):\n",
    "    LOGGER.handlers = []\n",
    "    LOGGER.setLevel(min(stderr_level, file_level))\n",
    "\n",
    "    if stderr:\n",
    "        handler = logging.StreamHandler(sys.stderr)\n",
    "        handler.setFormatter(FORMATTER)\n",
    "        handler.setLevel(stderr_level)\n",
    "        LOGGER.addHandler(handler)\n",
    "\n",
    "    if out_file is not None:\n",
    "        handler = logging.FileHandler(out_file)\n",
    "        handler.setFormatter(FORMATTER)\n",
    "        handler.setLevel(file_level)\n",
    "        LOGGER.addHandler(handler)\n",
    "\n",
    "    LOGGER.info(\"logger set up\")\n",
    "    return LOGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "def train_lgbm(X_train, y_train, X_valid, y_valid, X_test, categorical_features,lgb_params,fit_params, model_name,\n",
    "               loss_func, rank=False, calc_importances=True):\n",
    "    train = lgb.Dataset(X_train, y_train,categorical_feature=categorical_features)\n",
    "    if X_valid is not None:\n",
    "        valid = lgb.Dataset(X_valid, y_valid,categorical_feature=categorical_features)\n",
    "    evals_result = {}\n",
    "    if X_valid is not None:\n",
    "        model = lgb.train(\n",
    "            lgb_params,\n",
    "            train,\n",
    "            valid_sets=[valid],\n",
    "            valid_names=['valid'],\n",
    "            evals_result=evals_result,\n",
    "            **fit_params\n",
    "        )\n",
    "    else:\n",
    "        model = lgb.train(\n",
    "            lgb_params,\n",
    "            train,\n",
    "            evals_result=evals_result,\n",
    "            **fit_params\n",
    "        )\n",
    "    LOGGER.info(f'Best Iteration: {model.best_iteration}')\n",
    "\n",
    "    # train score\n",
    "    if X_valid is None:\n",
    "        y_pred_train = model.predict(X_train, num_iteration=fit_params[\"num_boost_round\"])\n",
    "        y_pred_train[y_pred_train<0] = 0\n",
    "        train_loss = loss_func(y_train, y_pred_train)\n",
    "    else:\n",
    "        y_pred_train = model.predict(X_train, num_iteration=model.best_iteration)\n",
    "        y_pred_train[y_pred_train < 0] = 0\n",
    "        train_loss = loss_func(y_train, y_pred_train)\n",
    "\n",
    "    if X_valid is not None:\n",
    "        # validation score\n",
    "        y_pred_valid = model.predict(X_valid)\n",
    "        y_pred_valid[y_pred_valid < 0] = 0\n",
    "        valid_loss = loss_func(y_valid, y_pred_valid)\n",
    "        # save prediction\n",
    "        np.save(f'{model_name}_train.npy', y_pred_valid)\n",
    "    else:\n",
    "        y_pred_valid = None\n",
    "        valid_loss = None\n",
    "\n",
    "    # save model\n",
    "    model.save_model(f'{model_name}.txt')\n",
    "\n",
    "    if X_test is not None:\n",
    "        # predict test\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        y_pred_test[y_pred_test < 0] = 0\n",
    "        # save prediction\n",
    "        np.save(f'{model_name}.npy', y_pred_test)\n",
    "    else:\n",
    "        y_pred_test = None\n",
    "\n",
    "    if calc_importances:\n",
    "        importances = pd.DataFrame()\n",
    "        importances['feature'] = feature_name\n",
    "        importances['gain'] = model.feature_importance(importance_type='gain')\n",
    "        importances['split'] = model.feature_importance(importance_type='split')\n",
    "    else:\n",
    "        importances = None\n",
    "\n",
    "    return y_pred_valid, y_pred_test, train_loss, valid_loss, importances, model.best_iteration\n",
    "\n",
    "def train_cat(X_train, y_train, X_valid, y_valid, X_test, categorical_features,model_name,loss_func,rank=False):\n",
    "\n",
    "    train = Pool(X_train, y_train, cat_features=categorical_features)\n",
    "    valid = Pool(X_valid, y_valid, cat_features=categorical_features)\n",
    "    evals_result = {}\n",
    "    model = CatBoostRegressor(random_seed=0, learning_rate=0.1,num_boost_round = 5000,loss_function='RMSE')\n",
    "    model.fit(train,\n",
    "              eval_set=valid,  # 検証用データ\n",
    "              early_stopping_rounds=1000,  # 10回以上精度が改善しなければ中止\n",
    "              verbose= 100,\n",
    "              use_best_model=True,  # 最も精度が高かったモデルを使用するかの設定\n",
    "              plot=False)  # 誤差の推移を描画するか否かの設定\n",
    "\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_train[y_pred_train<0] = 0\n",
    "    train_loss = loss_func(y_train, y_pred_train)\n",
    "    if X_valid is not None:\n",
    "        # validation score\n",
    "        y_pred_valid = model.predict(X_valid)\n",
    "        y_pred_valid[y_pred_valid < 0] = 0\n",
    "        valid_loss = loss_func(y_valid, y_pred_valid)\n",
    "        # save prediction\n",
    "        np.save(f'{model_name}.npy', y_pred_valid)\n",
    "    else:\n",
    "        y_pred_valid = None\n",
    "        valid_loss = None\n",
    "\n",
    "    # save model\n",
    "    model.save_model(f'{model_name}.txt')\n",
    "\n",
    "    if X_test is not None:\n",
    "        # predict test\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        y_pred_test[y_pred_test < 0] = 0\n",
    "        # save prediction\n",
    "        np.save(f'{model_name}.npy', y_pred_test)\n",
    "    else:\n",
    "        y_pred_test = None\n",
    "\n",
    "    return y_pred_valid, y_pred_test, train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここから始まり"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train,test準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./master.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df[\"target_flag\"]==0]\n",
    "test = df[df[\"target_flag\"]==1]\n",
    "train.to_csv(\"./train.csv\")\n",
    "test.to_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_train = pd.read_csv('./train.csv')[-8000000:]#ここを6月のみ取るようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampling_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"./test.csv\")\n",
    "y = sampling_train['imp'].copy()\n",
    "n_train = len(sampling_train)\n",
    "concat_train_test = sampling_train.append(test).reset_index(drop=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "train = copy.deepcopy(concat_train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴量作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster_cf_idを分離してカテゴリー特徴に"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feature]: make age feature...\n",
      "[feature]: make gender feature...\n",
      "[feature]: make cf feature...\n",
      "[feature]: join features...\n"
     ]
    }
   ],
   "source": [
    "print('[feature]: make age feature...')\n",
    "d = {\n",
    "    1 : 'u20', 2 : 'u20', 3 : 'u20', 4 : 'u20', 5 : 'u20'\n",
    "    , 6 : '20~34', 7 : '20~34', 8 : '20~34', 9 : '20~34', 10 : '20~34'\n",
    "    , 11 : '35u', 12 : '35u', 13 : '35u', 14 : '35u', 15 : '35u'\n",
    "    , 16 : '20~34', 17 : '20~34', 18 : '20~34', 19 : '20~34', 20 : '20~34'\n",
    "    , 21 : '35u', 22 : '35u', 23 : '35u', 24 : '35u', 25 : '35u'\n",
    "    , 26 : 'null', 27 : 'null', 28 : 'null', 29 : 'null', 30 : 'null'\n",
    "}\n",
    "age_df = pd.DataFrame(d.values(), index=d.keys()).reset_index()\n",
    "age_df.columns = ['cluster_cf_id', 'age']\n",
    "\n",
    "print('[feature]: make gender feature...')\n",
    "d = {\n",
    "1 : 'null', 2 : 'null', 3 : 'null', 4 : 'null', 5 : 'null'\n",
    ", 6 : 'm', 7 : 'm', 8 : 'm', 9 : 'm', 10 : 'm'\n",
    ", 11 : 'm', 12 : 'm', 13 : 'm', 14 : 'm', 15 : 'm'\n",
    ", 16 : 'f', 17 : 'f', 18 : 'f', 19 : 'f', 20 : 'f'\n",
    ", 21 : 'f', 22 : 'f', 23 : 'f', 24 : 'f', 25 : 'f'\n",
    ", 26 : 'null', 27 : 'null', 28 : 'null', 29 : 'null', 30 : 'null'\n",
    "}\n",
    "gender_df = pd.DataFrame(d.values(), index=d.keys()).reset_index()\n",
    "gender_df.columns = ['cluster_cf_id', 'gender']\n",
    "\n",
    "print('[feature]: make cf feature...')\n",
    "cf_dic = {\n",
    "1 : 'very little', 2 : 'little', 3 : 'normal', 4 : 'much', 5 : 'very much'\n",
    ", 6 : 'very little', 7 : 'little', 8 : 'normal', 9 : 'much', 10 : 'very much'\n",
    ", 11 : 'very little', 12 : 'little', 13 : 'normal', 14 : 'much', 15 : 'very much'\n",
    ", 16 : 'very little', 17 : 'little', 18 : 'normal', 19 : 'much', 20 : 'very much'\n",
    ", 21 : 'very little', 22 : 'little', 23 : 'normal', 24 : 'much', 25 : 'very much'\n",
    ", 26 : 'very little', 27 : 'little', 28 : 'normal', 29 : 'much', 30 : 'very much'\n",
    "}\n",
    "cf_df = pd.DataFrame(d.values(), index=d.keys()).reset_index()\n",
    "cf_df.columns = ['cluster_cf_id', 'cf']\n",
    "\n",
    "print('[feature]: join features...')\n",
    "train = pd.merge(train, age_df, on='cluster_cf_id', how='left')\n",
    "train = pd.merge(train, gender_df, on='cluster_cf_id', how='left')\n",
    "train = pd.merge(train, cf_df, on='cluster_cf_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日付データに対して日にち等を分離、かつsin,cosで連続値に"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"cm_start_at\"] = pd.to_datetime(train[\"cm_start_at\"])\n",
    "train[\"date_day\"] = train[\"cm_start_at\"].dt.day\n",
    "train[\"date_week\"] = train[\"cm_start_at\"].dt.week\n",
    "train[\"date_dayofweek\"] = train[\"cm_start_at\"].dt.dayofweek\n",
    "train[\"date_hour\"] = train[\"cm_start_at\"].dt.hour\n",
    "sincos = SinCos(feature_name=\"date_dayofweek\", period=6)\n",
    "train, _ = sincos.create_features(train)\n",
    "sincos = SinCos(feature_name=\"date_day\", period=30)\n",
    "train, _ = sincos.create_features(train)\n",
    "sincos = SinCos(feature_name=\"date_week\", period=6)\n",
    "train, _ = sincos.create_features(train)\n",
    "sincos = SinCos(feature_name=\"date_hour\", period=23)\n",
    "train, _ = sincos.create_features(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後の特徴量作成のために準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"date_hour\",\"date_dayofweek\",\"date_week\",\"date_day\",\"cf\",\"gender\",\"age\",\"cue_point_sequence\",\"cue_point_role\",\"channel_id\",\"genre_id\",\"series_id\",\"series_title\",\"specified\",\"lived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======0/15=====\n",
      "======1/15=====\n",
      "======2/15=====\n",
      "======3/15=====\n",
      "======4/15=====\n",
      "======5/15=====\n",
      "======6/15=====\n",
      "======7/15=====\n",
      "======8/15=====\n",
      "======9/15=====\n",
      "======10/15=====\n",
      "======11/15=====\n",
      "======12/15=====\n",
      "======13/15=====\n",
      "======14/15=====\n"
     ]
    }
   ],
   "source": [
    "target = Target(categorical_features,\"imp\")\n",
    "train, new_cols = target.create_features(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======0/15=====\n",
      "======1/15=====\n",
      "======2/15=====\n",
      "======3/15=====\n",
      "======4/15=====\n",
      "======5/15=====\n",
      "======6/15=====\n",
      "======7/15=====\n",
      "======8/15=====\n",
      "======9/15=====\n",
      "======10/15=====\n",
      "======11/15=====\n",
      "======12/15=====\n",
      "======13/15=====\n",
      "======14/15=====\n"
     ]
    }
   ],
   "source": [
    "frequency = Frequency(categorical_features)\n",
    "train, new_cols = frequency.create_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータ設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "LGBM_PARAMS = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': \"rmse\",\n",
    "    'learning_rate': 0.1,\n",
    "    'verbose': -1,\n",
    "    'nthread': -1,\n",
    "    'seed': SEED,\n",
    "}\n",
    "LGBM_FIT_PARAMS = {\n",
    "    'num_boost_round': 5000,\n",
    "    'early_stopping_rounds': 1000,\n",
    "    'verbose_eval': 500,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カテゴリー変数を数字にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======0/15=====\n",
      "======1/15=====\n",
      "======2/15=====\n",
      "======3/15=====\n",
      "======4/15=====\n",
      "======5/15=====\n",
      "======6/15=====\n",
      "======7/15=====\n",
      "======8/15=====\n",
      "======9/15=====\n",
      "======10/15=====\n",
      "======11/15=====\n",
      "======12/15=====\n",
      "======13/15=====\n",
      "======14/15=====\n"
     ]
    }
   ],
   "source": [
    "for index,c in enumerate(categorical_features):\n",
    "    print(\"======{}/{}=====\".format(index,len(categorical_features)))\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train[c].astype(\"str\").values))\n",
    "    train[c] = lbl.transform(list(train[c].astype(\"str\").values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いらないカラムを落とす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_drop_cols = [\"campaign_id\",\"cue_point_id\",\"cm_start_at\",\"imp\",\"Unnamed: 0\",\"target_flag\",\"cluster_cf_id\"]\n",
    "train = train.drop(train_drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train,test分離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = train[n_train:]\n",
    "train_train = train[:n_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train_train,y,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train:rightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds\n",
      "[500]\tvalid's rmse: 212.483\n",
      "[1000]\tvalid's rmse: 213.629\n",
      "Early stopping, best iteration is:\n",
      "[355]\tvalid's rmse: 212.148\n"
     ]
    }
   ],
   "source": [
    "y_pred_valid, y_pred_test, train_loss, valid_loss, importances, best_iter = train_lgbm(\n",
    "                x_train, y_train, x_valid, y_valid, train_test,\n",
    "                categorical_features=categorical_features,\n",
    "                lgb_params=LGBM_PARAMS,\n",
    "                fit_params=LGBM_FIT_PARAMS,\n",
    "                loss_func=calc_loss,\n",
    "                model_name = \"first_lightgbm\",\n",
    "                rank=False,\n",
    "                calc_importances=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train:catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 255.7499711\ttest: 259.7253080\tbest: 259.7253080 (0)\ttotal: 2.64s\tremaining: 3h 40m 6s\n",
      "100:\tlearn: 212.7577879\ttest: 218.3891794\tbest: 218.3108245 (98)\ttotal: 3m 50s\tremaining: 3h 5m 57s\n",
      "200:\tlearn: 209.0461561\ttest: 216.1953789\tbest: 216.1953789 (200)\ttotal: 7m 49s\tremaining: 3h 6m 53s\n",
      "300:\tlearn: 206.5847620\ttest: 215.4191065\tbest: 215.3752401 (297)\ttotal: 11m 57s\tremaining: 3h 6m 45s\n",
      "400:\tlearn: 204.7432490\ttest: 215.1324990\tbest: 214.9976273 (373)\ttotal: 16m 1s\tremaining: 3h 3m 44s\n",
      "500:\tlearn: 202.8132268\ttest: 214.8505865\tbest: 214.8241692 (495)\ttotal: 20m 23s\tremaining: 3h 3m 5s\n"
     ]
    }
   ],
   "source": [
    "y_pred_valid, y_pred_test, train_loss, valid_loss=train_cat(x_train, y_train, x_valid, y_valid,test,categorical_features,\n",
    "                                                            model_name=\"first_catboost\",loss_func=calc_loss,rank=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUBMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"./test.csv\")\n",
    "sub['imp'] = y_pred_test\n",
    "sub[[\"cue_point_id\",\"cue_point_sequence\",\"cluster_cf_id\",\"imp\"]].to_csv('submission_cat.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
